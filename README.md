# Machine Translation Evaluation Framework

[![DOI](https://zenodo.org/badge/DOI/10.5281/zenodo.10256836.svg)](https://doi.org/10.5281/zenodo.10256836)

---

## About this lm-evaluation-harness fork

This fork of the EleutherAI's [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) aims to be used as the evaluation framework for machine translation-related tasks. This fork is maintained by the Language Technologies unit wihin the Barcelona Supercomputing Center (BSC).


## Contents

- [Installation](#installation)
- [Getting started](#getting-started)
  - [Supported models](#supported-models)
  - [Tasks](#tasks)
    - [Run a task](#run-a-task)
  - [Metrics](#metrics)
- [Visual interface](#visual-interface)
- [Citation](#citation)

---

## Installation

To use our framework first clone the project by:

```bash
git clone https://github.com/langtech-bsc/mt-evaluation.git
```

Then install the required dependencies:

```bash
cd mt-evaluation
pip install -e .
```

---

## Getting started

### Supported models


### Tasks

#### Run a task

prompt_style

### Metrics


## Visual Interface

This evaluation framework includes a visual interface for easy result exploration and comparison of models. To launch the interface, run:

```bash
```

## Citation

If you use this framework in your research, please cite it as follows: